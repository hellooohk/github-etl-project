# ğŸš€ GitHub Repositories ETL Pipeline

A modular ETL (Extract, Transform, Load) pipeline that fetches top GitHub repositories based on a topic, processes the data, and stores it into a local SQLite database. Built with best industry practices in mind.

---

## ğŸ›  Tech Stack

| Purpose         | Tool                      |
|-----------------|---------------------------|
| Language        | Python 3.x                |
| Data Extraction | GitHub REST API           |
| Data Storage    | SQLite                    |
| Config & Secrets| `.env` + `dotenv`         |
| Logging         | `logging` + `colorama`    |
| Structure       | Modular Python packages   |

---

## ğŸ“ Project Structure

```

github\_etl\_project/
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py         # Extracts data from GitHub API
â”‚   â”œâ”€â”€ transform.py       # Cleans and transforms raw data
â”‚   â””â”€â”€ load.py            # Loads cleaned data into SQLite
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.py        # Configuration and constants
â”‚   â””â”€â”€ logger.py          # Colorful and file-based logging setup
â”‚
â”œâ”€â”€ db/
â”‚   â””â”€â”€ schema.sql         # (Optional) SQL schema if needed
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ run.log            # Log file (auto-generated)
â”‚
â”œâ”€â”€ main.py                # Entrypoint to run the ETL job
â”œâ”€â”€ .env                   # Secrets and environment variables
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # Project overview

````

---

## âš™ï¸ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/your-username/github-etl-project.git
cd github-etl-project
````

### 2. Set up a virtual environment

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure environment variables

Create a `.env` file in the root:

```env
TOPIC=data engineering
```

You can change `TOPIC` to anything like `"react"`, `"machine learning"`, etc.

---

## ğŸš€ Running the ETL Pipeline

```bash
python main.py
```

This will:

* Extract top 10 repositories from GitHub for the given topic
* Transform the data into clean structure
* Load it into a local SQLite database `db/github_repos.db`
* Log the whole process to `logs/run.log` and terminal (with colors!)

---

## ğŸ“Š Example Output

**Database (`db/github_repos.db`)**

| id       | full\_name     | stars | language   |
| -------- | -------------- | ----- | ---------- |
| 28457823 | facebook/react | 220k  | JavaScript |
| ...      | ...            | ...   | ...        |

**Log (`logs/run.log`)**

```
2025-05-20 13:21:10 - INFO - ETL job started.
2025-05-20 13:21:11 - INFO - Fetching from GitHub API...
2025-05-20 13:21:12 - INFO - Loading to DB...
2025-05-20 13:21:13 - INFO - ETL job completed.
```

---

## âœ… Best Practices Used

* ğŸ§± Modular Code (`extract`, `transform`, `load`)
* ğŸ” Config via `.env`
* ğŸ–¥ Logs to both terminal and file
* ğŸ¨ Colored CLI logs for easier debugging
* ğŸ“¦ Ready for Docker, Airflow, or cron extension
* ğŸ§ª Easily testable functions

---

## ğŸ“Œ Next Steps (Optional Enhancements)

* [ ] Add unit tests with `pytest`
* [ ] Switch to PostgreSQL or cloud database
* [ ] Schedule with Airflow or Prefect
* [ ] Dockerize the pipeline
* [ ] Add retry and exponential backoff to API calls

---

## ğŸ“„ License

MIT License. Free to use, share, and learn from.
